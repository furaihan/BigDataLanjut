{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Nama: Muhammad Zhafar Al Fathi\n",
        "## NIM: 21.11.4374\n",
        "## Kelas: 21-IF-08"
      ],
      "metadata": {
        "id": "2LOqno6y9D8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-aJlj9cGAD6",
        "outputId": "1bd0b36a-9cc9-4456-c58c-165358deaeb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=5356540402a8f5650cfbe458b3ff473f7898fcc6e003ae7d5cbad4b39088d4a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO__bfItHL6i",
        "outputId": "7abd9174-9a43-4a9c-85a1-8e1b53d35c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnXghkVyHOWQ",
        "outputId": "08a025c7-73cf-496b-d83f-90610561833b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import py4j\n",
        "print(py4j.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aabJBHAbAP_L",
        "outputId": "81491f10-c494-44a2-f583-6bd3afeeb2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10.9.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spark.stop()\n",
        "# creating Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark = SparkSession.builder\\\n",
        "        .appName(\"RDD\")\\\n",
        "        .config('spark.ui.port', '1453')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark\n",
        "sc = spark.sparkContext\n",
        "sc.setCheckpointDir('checkpoint/')"
      ],
      "metadata": {
        "id": "NqLCQ9qkHPvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FYF10OvzueXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.recommendation import Rating\n",
        "from pyspark.mllib.recommendation import ALS"
      ],
      "metadata": {
        "id": "a8qx_BM_HoqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = sc.textFile('/content/drive/MyDrive/File Kuliah/Tugas/Semester 5/Big Data & Predictive Analytics Lanjut/ml_ratings.txt')\n",
        "ratings = data.map(lambda l: l.split('::'))"
      ],
      "metadata": {
        "id": "9sYh8WpSH14y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in ratings.take(10):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "jTrPI-lZIJHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf28841f-be77-4677-c169-1606fc7ddca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0', '2', '3', '1424380312']\n",
            "['0', '3', '1', '1424380312']\n",
            "['0', '5', '2', '1424380312']\n",
            "['0', '9', '4', '1424380312']\n",
            "['0', '11', '1', '1424380312']\n",
            "['0', '12', '2', '1424380312']\n",
            "['0', '15', '1', '1424380312']\n",
            "['0', '17', '1', '1424380312']\n",
            "['0', '19', '1', '1424380312']\n",
            "['0', '21', '1', '1424380312']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = ratings_final.randomSplit([0.9, 0.1], seed=42)\n",
        "train_size = train.count()\n",
        "test_size = test.count()\n",
        "\n",
        "print(\"Size of train set:\", train_size)\n",
        "print(\"Size of test set:\", test_size)"
      ],
      "metadata": {
        "id": "HwJ6jI0LI-Ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b37521-df65-45c8-f3e8-5ac9e2fe7ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of train set: 1353\n",
            "Size of test set: 148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_calculate_mse(train_data, test_data, rank, iterations, seed):\n",
        "    # Inisialisasi model ALS\n",
        "    als = ALS()\n",
        "\n",
        "    # Melatih model ALS menggunakan data pelatihan dengan parameter yang diberikan\n",
        "    model = als.train(train_data, rank=rank, iterations=iterations, seed=seed)\n",
        "\n",
        "    # Melakukan prediksi untuk data testing\n",
        "    predict = model.predictAll(test_data.map(lambda x: [x[0], x[1]]))\n",
        "\n",
        "    # Mengubah format data untuk rates (data aktual) dan preds (prediksi) supaya bisa di-join\n",
        "    rates = ratings_final.map(lambda r: ((r[0], r[1]), r[2]))\n",
        "    preds = predict.map(lambda r: ((r[0], r[1]), r[2]))\n",
        "\n",
        "    # Menggabungkan data aktual dan prediksi berdasarkan key (user, item)\n",
        "    rates_and_preds = rates.join(preds)\n",
        "\n",
        "    # Menghitung Mean Squared Error (MSE)\n",
        "    MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
        "\n",
        "    # Mengembalikan nilai MSE sebagai hasil fungsi\n",
        "    return MSE\n"
      ],
      "metadata": {
        "id": "fHTYTOYVKJY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = train_and_calculate_mse(train, test, 6, 9, seed=4374)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL_4yLE9FuF2",
        "outputId": "f16d807f-6e0d-4c57-8a3a-debf7cbcc001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.9761489311059669\n"
          ]
        }
      ]
    }
  ]
}