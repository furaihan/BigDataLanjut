{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Nama: Muhammad Zhafar Al Fathi\n",
        "### NIM: 21.11.4374\n",
        "### Kelas: 21-IF-08"
      ],
      "metadata": {
        "id": "pvFZ_TcvuBHN"
      },
      "id": "pvFZ_TcvuBHN"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfR8ltxaCzs6",
        "outputId": "2e8a1828-fd9b-4c5b-943d-abb960312db7"
      },
      "id": "pfR8ltxaCzs6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=bab6294540e8bc43106e34666308234f01445253d85127cb120e08591cb7c3d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxdbCqxmDRGr",
        "outputId": "c9febb33-153b-43df-cf15-977d593ef4c8"
      },
      "id": "JxdbCqxmDRGr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy8pGqRxCzN7",
        "outputId": "b9bb0e74-4672-4b03-9aad-434f20a74d21"
      },
      "id": "Gy8pGqRxCzN7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "398d0254",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "398d0254",
        "outputId": "7ad2cf83-61d8-400f-f600-1e4b1b1e41ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.10/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "969a0797",
      "metadata": {
        "id": "969a0797"
      },
      "outputs": [],
      "source": [
        "# spark.stop()\n",
        "# creating Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"RDD\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pddf = pd.read_csv(\"/content/drive/MyDrive/File Kuliah/Tugas/Semester 5/Big Data & Predictive Analytics Lanjut/diabetes.csv\")\n",
        "pddf.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBJ1Y4IcFvQT",
        "outputId": "f51cd23e-cadf-434c-f33e-44d12a52c7ea"
      },
      "id": "QBJ1Y4IcFvQT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Pregnancies               768 non-null    int64  \n",
            " 1   Glucose                   768 non-null    int64  \n",
            " 2   BloodPressure             768 non-null    int64  \n",
            " 3   SkinThickness             768 non-null    int64  \n",
            " 4   Insulin                   768 non-null    int64  \n",
            " 5   BMI                       768 non-null    float64\n",
            " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
            " 7   Age                       768 non-null    int64  \n",
            " 8   Outcome                   768 non-null    int64  \n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3f7ccd0",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3f7ccd0",
        "outputId": "7aa2ece8-2521-4ad1-e5ef-8c46e451ef21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
            "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|\n",
            "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
            "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|\n",
            "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|\n",
            "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|\n",
            "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|      0|\n",
            "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|      1|\n",
            "|          5|    116|           74|            0|      0|25.6|                   0.201| 30|      0|\n",
            "|          3|     78|           50|           32|     88|31.0|                   0.248| 26|      1|\n",
            "|         10|    115|            0|            0|      0|35.3|                   0.134| 29|      0|\n",
            "|          2|    197|           70|           45|    543|30.5|                   0.158| 53|      1|\n",
            "|          8|    125|           96|            0|      0| 0.0|                   0.232| 54|      1|\n",
            "|          4|    110|           92|            0|      0|37.6|                   0.191| 30|      0|\n",
            "|         10|    168|           74|            0|      0|38.0|                   0.537| 34|      1|\n",
            "|         10|    139|           80|            0|      0|27.1|                   1.441| 57|      0|\n",
            "|          1|    189|           60|           23|    846|30.1|                   0.398| 59|      1|\n",
            "|          5|    166|           72|           19|    175|25.8|                   0.587| 51|      1|\n",
            "|          7|    100|            0|            0|      0|30.0|                   0.484| 32|      1|\n",
            "|          0|    118|           84|           47|    230|45.8|                   0.551| 31|      1|\n",
            "|          7|    107|           74|            0|      0|29.6|                   0.254| 31|      1|\n",
            "|          1|    103|           30|           38|     83|43.3|                   0.183| 33|      0|\n",
            "|          1|    115|           70|           30|     96|34.6|                   0.529| 32|      1|\n",
            "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv(\"/content/drive/MyDrive/File Kuliah/Tugas/Semester 5/Big Data & Predictive Analytics Lanjut/diabetes.csv\", header=True, inferSchema=True)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb12f09",
      "metadata": {
        "id": "bdb12f09"
      },
      "outputs": [],
      "source": [
        "data = df.select('Glucose')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05a581b9",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05a581b9",
        "outputId": "76621212-b1e5-4aae-a03d-9a3855feb27c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|Glucose|\n",
            "+-------+\n",
            "|    148|\n",
            "|     85|\n",
            "|    183|\n",
            "|     89|\n",
            "|    137|\n",
            "|    116|\n",
            "|     78|\n",
            "|    115|\n",
            "|    197|\n",
            "|    125|\n",
            "|    110|\n",
            "|    168|\n",
            "|    139|\n",
            "|    189|\n",
            "|    166|\n",
            "|    100|\n",
            "|    118|\n",
            "|    107|\n",
            "|    103|\n",
            "|    115|\n",
            "+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "feature_columns = df.columns[:-1]\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "data = assembler.transform(df)"
      ],
      "metadata": {
        "id": "7KsHGnkjIq2y"
      },
      "id": "7KsHGnkjIq2y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYCCc2M9hZrz",
        "outputId": "48c351ae-71da-4237-a7bd-16493806c069"
      },
      "id": "xYCCc2M9hZrz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+--------------------+\n",
            "|Pregnancies|Glucose|BloodPressure|SkinThickness|Insulin| BMI|DiabetesPedigreeFunction|Age|Outcome|            features|\n",
            "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+--------------------+\n",
            "|          6|    148|           72|           35|      0|33.6|                   0.627| 50|      1|[6.0,148.0,72.0,3...|\n",
            "|          1|     85|           66|           29|      0|26.6|                   0.351| 31|      0|[1.0,85.0,66.0,29...|\n",
            "|          8|    183|           64|            0|      0|23.3|                   0.672| 32|      1|[8.0,183.0,64.0,0...|\n",
            "|          1|     89|           66|           23|     94|28.1|                   0.167| 21|      0|[1.0,89.0,66.0,23...|\n",
            "|          0|    137|           40|           35|    168|43.1|                   2.288| 33|      1|[0.0,137.0,40.0,3...|\n",
            "|          5|    116|           74|            0|      0|25.6|                   0.201| 30|      0|[5.0,116.0,74.0,0...|\n",
            "|          3|     78|           50|           32|     88|31.0|                   0.248| 26|      1|[3.0,78.0,50.0,32...|\n",
            "|         10|    115|            0|            0|      0|35.3|                   0.134| 29|      0|[10.0,115.0,0.0,0...|\n",
            "|          2|    197|           70|           45|    543|30.5|                   0.158| 53|      1|[2.0,197.0,70.0,4...|\n",
            "|          8|    125|           96|            0|      0| 0.0|                   0.232| 54|      1|[8.0,125.0,96.0,0...|\n",
            "|          4|    110|           92|            0|      0|37.6|                   0.191| 30|      0|[4.0,110.0,92.0,0...|\n",
            "|         10|    168|           74|            0|      0|38.0|                   0.537| 34|      1|[10.0,168.0,74.0,...|\n",
            "|         10|    139|           80|            0|      0|27.1|                   1.441| 57|      0|[10.0,139.0,80.0,...|\n",
            "|          1|    189|           60|           23|    846|30.1|                   0.398| 59|      1|[1.0,189.0,60.0,2...|\n",
            "|          5|    166|           72|           19|    175|25.8|                   0.587| 51|      1|[5.0,166.0,72.0,1...|\n",
            "|          7|    100|            0|            0|      0|30.0|                   0.484| 32|      1|[7.0,100.0,0.0,0....|\n",
            "|          0|    118|           84|           47|    230|45.8|                   0.551| 31|      1|[0.0,118.0,84.0,4...|\n",
            "|          7|    107|           74|            0|      0|29.6|                   0.254| 31|      1|[7.0,107.0,74.0,0...|\n",
            "|          1|    103|           30|           38|     83|43.3|                   0.183| 33|      0|[1.0,103.0,30.0,3...|\n",
            "|          1|    115|           70|           30|     96|34.6|                   0.529| 32|      1|[1.0,115.0,70.0,3...|\n",
            "+-----------+-------+-------------+-------------+-------+----+------------------------+---+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = data.randomSplit([0.9, 0.1], seed=9876755)"
      ],
      "metadata": {
        "id": "F7ifrD9MeuPT"
      },
      "id": "F7ifrD9MeuPT",
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression = LogisticRegression(featuresCol=\"features\", labelCol=\"Outcome\")\n",
        "model = logistic_regression.fit(train_data)"
      ],
      "metadata": {
        "id": "WXVNy2mse3XN"
      },
      "id": "WXVNy2mse3XN",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.transform(test_data)"
      ],
      "metadata": {
        "id": "6MVzIRw9fBme"
      },
      "id": "6MVzIRw9fBme",
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using AUC\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Outcome\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "# Print the AUC score\n",
        "print(\"Area Under ROC Curve (AUC):\", auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkmRaW6YfITu",
        "outputId": "9e4ad9f9-f94a-4d50-dd4a-4b37326114ae"
      },
      "id": "SkmRaW6YfITu",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC Curve (AUC): 0.8151111111111112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19fe22fd",
      "metadata": {
        "id": "19fe22fd"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "5b58183a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b58183a",
        "outputId": "6e684987-a2c0-4af9-dc56-c841459a4ecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(20,[6,8,13,16],[...|\n",
            "|  0.0|(20,[0,2,7,13,15,...|\n",
            "|  1.0|(20,[3,4,6,11,19]...|\n",
            "+-----+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0.0, \"Hi I heard about Spark\"),\n",
        "    (0.0, \"I wish Java could use case classes\"),\n",
        "    (1.0, \"Logistic regression models are neat\")\n",
        "], [\"label\", \"sentence\"])\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "\n",
        "rescaledData.select(\"label\", \"features\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29dd0b32",
      "metadata": {
        "id": "29dd0b32"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "0f0809b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f0809b3",
        "outputId": "abbc2e12-a55b-4675-c2a3-999f54b0c37e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: [Hi, I, heard, about, Spark] => \n",
            "Vector: [-0.002007347345352173,-0.01002005971968174,0.014816772192716599]\n",
            "\n",
            "Text: [I, wish, Java, could, use, case, classes] => \n",
            "Vector: [0.05740070236580712,0.05394199650202478,0.0715796984732151]\n",
            "\n",
            "Text: [Logistic, regression, models, are, neat] => \n",
            "Vector: [0.0329450573772192,-0.0382571728900075,-0.03537467569112778]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Word2Vec\n",
        "\n",
        "# Input data: Each row is a bag of words from a sentence or document.\n",
        "documentDF = spark.createDataFrame([\n",
        "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
        "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
        "    (\"Logistic regression models are neat\".split(\" \"), )\n",
        "], [\"text\"])\n",
        "\n",
        "# Learn a mapping from words to Vectors.\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
        "model = word2Vec.fit(documentDF)\n",
        "\n",
        "result = model.transform(documentDF)\n",
        "for row in result.collect():\n",
        "    text, vector = row\n",
        "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c39c3eba",
      "metadata": {
        "id": "c39c3eba"
      },
      "source": [
        "## Countvectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "7f36ee49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f36ee49",
        "outputId": "ea8c4eec-fb41-405b-ab1f-af5438e4e571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------+-------------------------+\n",
            "|id |words          |features                 |\n",
            "+---+---------------+-------------------------+\n",
            "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
            "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
            "+---+---------------+-------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "# Input data: Each row is a bag of words with an ID.\n",
        "df = spark.createDataFrame([\n",
        "    (0, \"a b c\".split(\" \")),\n",
        "    (1, \"a b b c a\".split(\" \"))\n",
        "], [\"id\", \"words\"])\n",
        "\n",
        "# fit a CountVectorizerModel from the corpus.\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
        "\n",
        "model = cv.fit(df)\n",
        "\n",
        "result = model.transform(df)\n",
        "result.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88906a86",
      "metadata": {
        "id": "88906a86"
      },
      "source": [
        "## PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "f9a0d17b",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9a0d17b",
        "outputId": "535d41f3-0168-4ec5-f954-bcded30f0a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(id=4, text='spark i j k', words=['spark', 'i', 'j', 'k'], features=SparseVector(262144, {19036: 1.0, 68693: 1.0, 173558: 1.0, 213660: 1.0}), rawPrediction=DenseVector([0.5288, -0.5288]), probability=DenseVector([0.6292, 0.3708]), prediction=0.0), Row(id=5, text='l m n', words=['l', 'm', 'n'], features=SparseVector(262144, {1303: 1.0, 52644: 1.0, 248090: 1.0}), rawPrediction=DenseVector([4.1691, -4.1691]), probability=DenseVector([0.9848, 0.0152]), prediction=0.0), Row(id=6, text='spark hadoop spark', words=['spark', 'hadoop', 'spark'], features=SparseVector(262144, {173558: 2.0, 198017: 1.0}), rawPrediction=DenseVector([-1.865, 1.865]), probability=DenseVector([0.1341, 0.8659]), prediction=1.0), Row(id=7, text='apache hadoop', words=['apache', 'hadoop'], features=SparseVector(262144, {68303: 1.0, 198017: 1.0}), rawPrediction=DenseVector([5.4156, -5.4156]), probability=DenseVector([0.9956, 0.0044]), prediction=0.0)]\n",
            "(4, spark i j k) --> prob=[0.6292098489668484,0.3707901510331516], prediction=0.000000\n",
            "(5, l m n) --> prob=[0.984770006762304,0.015229993237696027], prediction=0.000000\n",
            "(6, spark hadoop spark) --> prob=[0.13412348342566158,0.8658765165743384], prediction=1.000000\n",
            "(7, apache hadoop) --> prob=[0.9955732114398529,0.00442678856014711], prediction=0.000000\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "\n",
        "# Prepare training documents from a list of (id, text, label) tuples.\n",
        "training = spark.createDataFrame([\n",
        "    (0, \"a b c d e spark\", 1.0),\n",
        "    (1, \"b d\", 0.0),\n",
        "    (2, \"spark f g h\", 1.0),\n",
        "    (3, \"hadoop mapreduce\", 0.0)\n",
        "], [\"id\", \"text\", \"label\"])\n",
        "\n",
        "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
        "\n",
        "# Fit the pipeline to training documents.\n",
        "model = pipeline.fit(training)\n",
        "\n",
        "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
        "test = spark.createDataFrame([\n",
        "    (4, \"spark i j k\"),\n",
        "    (5, \"l m n\"),\n",
        "    (6, \"spark hadoop spark\"),\n",
        "    (7, \"apache hadoop\")\n",
        "], [\"id\", \"text\"])\n",
        "\n",
        "# Make predictions on test documents and print columns of interest.\n",
        "prediction = model.transform(test)\n",
        "print (prediction.collect())\n",
        "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
        "for row in selected.collect():\n",
        "    rid, text, prob, prediction = row\n",
        "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "801bb53b",
      "metadata": {
        "id": "801bb53b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.6 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "5bc067213193d7d391907c464a8e66c6ece1a82666c1d2406921e2c8deb6cf3c"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}